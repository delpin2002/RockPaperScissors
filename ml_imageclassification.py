# -*- coding: utf-8 -*-
"""Dicoding_ML_ImageClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i79ukGsFxO828q9_A7R_1aRsljhLTCMo

# SUBMISSION

## Nama: Delvin Fachrizky

Kriteria submission yang harus dipenuhi:

1. Dataset yang dipakai haruslah dataset berikut : rockpaperscissors, atau gunakan link ini pada wget command: https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip.
2. Dataset harus dibagi menjadi train set dan validation set.
3. Ukuran validation set harus 40% dari total dataset (data training memiliki 1314 sampel, dan data validasi sebanyak 874 sampel).
4. Harus mengimplementasikan augmentasi gambar.
5. Menggunakan image data generator.
6. Model harus menggunakan model sequential.
7. Pelatihan model tidak melebihi waktu 30 menit.
8. Program dikerjakan pada Google Colaboratory.
9. Akurasi dari model minimal 85%.
10. Dapat memprediksi gambar yang diunggah ke Colab seperti gambar di bawah.

#### **Import library yang dibutuhkan pada project ini**
"""

# Commented out IPython magic to ensure Python compatibility.
"""
  NOTE: Seluruh library yang digunakan/dibutuhkan akan ditempatkan pada cell ini
"""

%matplotlib inline
import os
import shutil
import zipfile
import numpy as np
import splitfolders
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from google.colab import files
from keras.preprocessing import image
from tensorflow.keras.optimizers import RMSprop
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Menginstall library split folder
!pip install split_folders

# Mengecek versi dari Tensorflow
print(tf.__version__)

# Mendownload dataset dari link yang diberikan
!wget --no-check-certificate https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip -O rockpaperscissors.zip

# Melakukan ekstraksi terhadap file zip
loc_zip = "rockpaperscissors.zip"
zip = zipfile.ZipFile(loc_zip, "r")
zip.extractall("RPS")
zip.close()

class call_back(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.97):
      print("\n Hetikan proses jika akurasi telah melebihi 97%")
      self.model.stop_training = True

callbacks = call_back()

"""#### **Memisahkan data training dan data validasi**"""

root_dir = '/content/RPS/rockpaperscissors'
train_dir = os.path.join(root_dir,'train')
validator_dir = os.path.join(root_dir, 'val')
rock_dir = os.path.join(root_dir,'rock')
paper_dir = os.path.join(root_dir, 'paper')
scissors_dir = os.path.join(root_dir, 'scissors')

os.mkdir(train_dir)
os.mkdir(validator_dir)

train_rock = os.path.join(train_dir, 'rock')
train_paper = os.path.join(train_dir, 'paper')
train_scissors = os.path.join(train_dir, 'scissors')
val_rock = os.path.join(validator_dir, 'rock')
val_paper = os.path.join(validator_dir, 'paper')
val_scissors = os.path.join(validator_dir, 'scissors')

paper_images = os.listdir(train_paper)
scissors_images = os.listdir(train_scissors)
rock_images = os.listdir(train_rock)

os.mkdir(train_rock)
os.mkdir(train_paper)
os.mkdir(train_scissors)
os.mkdir(val_rock)
os.mkdir(val_paper)
os.mkdir(val_scissors)

# Membagi direktori menjadi data train dan data validasi
train_roc_dir, val_roc_dir = train_test_split(os.listdir(rock_dir), test_size = 0.40)
train_pap_dir, val_pap_dir = train_test_split(os.listdir(paper_dir), test_size = 0.40)
train_sci_dir, val_sci_dir = train_test_split(os.listdir(scissors_dir), test_size = 0.40)

# Training
for file in train_roc_dir:
  shutil.copy(os.path.join(rock_dir, file), os.path.join(train_rock, file))
for file in train_pap_dir:
  shutil.copy(os.path.join(paper_dir,file), os.path.join(train_paper,file))
for file in train_sci_dir:
  shutil.copy(os.path.join(scissors_dir,file), os.path.join(train_scissors,file))

# Validation
for file in val_roc_dir:
  shutil.copy(os.path.join(rock_dir, file), os.path.join(val_rock,file))
for file in val_pap_dir:
  shutil.copy(os.path.join(paper_dir,file), os.path.join(val_paper,file))
for file in val_sci_dir:
  shutil.copy(os.path.join(scissors_dir,file), os.path.join(val_scissors,file))

# SCISSORS
plt.figure(figsize=(20, 3))
for count, img_path in enumerate(scissors_images[:3]):
  sp = plt.subplot(1, 3, count+1)
  img = mpimg.imread(os.path.join(train_scissors, img_path))
  plt.imshow(img)
plt.show()

# ROCK IMAGES
plt.figure(figsize=(20, 3))
for count, img_path in enumerate(rock_images[:3]):
  sp = plt.subplot(1, 3, count+1)
  img = mpimg.imread(os.path.join(train_rock, img_path))
  plt.imshow(img)
plt.show()

# PAPER IMAGES
plt.figure(figsize=(20, 3))
for count, img_path in enumerate(paper_images[:3]):
  sp = plt.subplot(1, 3, count+1)
  img = mpimg.imread(os.path.join(train_paper, img_path))
  plt.imshow(img)
plt.show()

"""#### **Augmentasi Data**

Augmentasi data adalah suatu proses dalam pengolahan data gambar, augmentasi merupakan proses mengu- bah atau memodifikasi gambar sedemikian rupa sehingga komputer akan mendeteksi bahwa gambar yang diubah adalah gambar yang berbeda, namun manusia masih dapat mengetahui bahwa gambar yang diubah tersebut adalah gambar
"""

#Augmentasi data gambar
train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 20,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest',
)
test_datagen = ImageDataGenerator(
    rescale = 1./225,
    rotation_range = 20,
    horizontal_flip = True,
    vertical_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest'
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150,150),
    batch_size= 32,
    class_mode='categorical'
)

validation_generator = test_datagen.flow_from_directory(
    validator_dir,
    target_size = (150,150),
    batch_size = 32,
    class_mode = 'categorical'
)

"""#### **Melatih model**"""

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape= (150,150,3)),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(64,(3,3), activation= 'relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(128,(3,3), activation= 'relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(512, activation= 'relu'),
  tf.keras.layers.Dense(3, activation= 'softmax')
])

model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

"""#### **Mengkompliasi Model**"""

#Kompilasi model dan alur pelatihan

model.compile(loss = 'categorical_crossentropy',
              optimizer= 'RMSprop', 
              metrics= ['accuracy'])

_history_ = model.fit(
    train_generator,
    steps_per_epoch = 41,
    epochs = 20,
    validation_data = validation_generator,
    validation_steps = 27,
    verbose =2,
    callbacks=[callbacks]
)

"""#### **Melihat Plot dari data training dan validasi**"""

acc = _history_.history['accuracy']
val_acc = _history_.history['val_accuracy']

loss = _history_.history['loss']
val_loss = _history_.history['val_loss']

plt.plot(acc, color='blue')
plt.plot(val_acc, color='green')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(loss, color='blue')
plt.plot(val_loss, color='green')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show

"""#### **Melakukan Percobaan Prediksi dengan Gambar**"""

uploaded = files.upload()

for fn in uploaded.keys():

  path = fn 
  img = image.load_img(path, target_size =(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)

  print(fn)
  if classes[0,0]!=0:
    print('paper')
  elif classes[0,1]!=0:
    print('rock')
  else:
    print('scissors')